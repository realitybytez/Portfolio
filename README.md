# Portfolio

This project demonstrates an implementation of Databricks' widely marketed medallion design pattern from scratch. 

Source data for this project is generated by a bespoke generator from an imaginary source system, "Policy Forge". The generation and modelling of policy data in the Insurance business domain via a policy administration system is mimicked using this generator. Data is pushed to a replica of the prod database and captured for source data acquisition purposes through a change data capture (CDC) process.  

The Azure cloud platform is utilised for source systems compute infrastructure and read only flat file storage (ADLS2) of the bronze layer, along with Bicep templates for automated deployment of Azure resources and Key Vault for secrets security. 

Snowflake is leveraged for data warehousing and manipulation, with the ability to query the bronze layer directly via external tables. Snowflake also hosts the silver and gold layers in the form of tables. 

Dagster is responsible for orchestration and DBT for curation of datasets along with CI/CD workflows.

The setup of almost all components is done via glue/automation code (mixture of Python and bash) and the guide below can be used to get up and running on a local or cloud resource.

[Temp note: This is still a work in progress - guide is helpful but not 100% complete. Silver DBT job needs to be kicked off manually with DBT build, silver & gold layers still in progress!]
1. Setup Cloud Services
```
# Create a fresh Azure subscription / use existing subscription
# Create a fresh Snowflake account / use existing account
```

2. Define Secrets
```
# Create an Azure Key Vault utilising RBAC
Give yourself the following roles:
Key Vault Administrator
Key Vault Secrets Officer

# Create a secret storing the password used for snowflake
Name: snowflake
Secret value: <choose a password>

# Create a secret storing the password to be used for an OS user on an Azure VM
Name: portfolioosuser
Secret value: <choose a password>

```

3. Clone this repo ~ (e.g. $HOME) to any linux box
```
git clone git@github.com:realitybytez/Portfolio.git
```

4. Install dependencies for setup scripts
``` 
sudo apt-get install jq
wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq
chmod +x /usr/bin/yq
```

5. Configure and run Azure setup scripts
```
# In this script
src/portfolio/cloud_infrastructure/azure_setup_user_permissions.sh

# Update the following variables
# subscription_id: Azure subscription ID 
# key_vault_name: Name of key vault created above

# In this script
src/portfolio/cloud_infrastructure/azure_deploy_infrastructure.sh

# Update the following variables as appropriate
tenant='4709ea8c-d3bd-4226-9769-cd360ea2e962' # Your tenant id
resource_group="Portfolio" # The resource group you will use
location="australiaeast" # Azure location you will use

# Log in to Azure CLI as the owner
az login
# Run the following
src/portfolio/cloud_infrastructure/azure_setup_user_permissions.sh
src/portfolio/cloud_infrastructure/azure_deploy_infrastructure.sh
```

6. Setup virtual environment & Python dependencies on local or Azure machine
```
# Run the following script
src/portfolio/cloud_infrastructure/setup_venv.sh
```

7. Setup postgres (hosts source data replica) on local or Azure machine
```
# Run src/portfolio/cloud_infrastructure/postgres_setup.sh
```

8. Enable Snowflake Integration
```

# In this yaml config
src/portfolio/cloud_infrastructure/shared_config.yml

# Update snowflake account details
snowflake:
  account: xxxxxxx-xx99999
  user: xxxxxxxx

# Run the following script
python3 ~/Portfolio/src/portfolio/cloud_infrastructure/snowflake_setup.py

# Run the following query
DESC INTEGRATION portfolio_bronze_layer;

# Take the value in AZURE_CONSENT_URL, open it in a browser and accept the prompt
# Note the value in AZURE_MULTI_TENANT_APP_NAME e.g cfbkjasnowflakepacint_1727079109755
# Take the name e.g cfbkjasnowflakepacint and give it the storage blob data reader role on
# the storage container blob store created by point 5
```

9. Build bronze data structures (external tables) in Snowflake
```
# Run the following script
src/portfolio/cloud_infrastructure/snowflake_populate_bronze.py
```

10. Turn Dagster on
```
# Run the following scripts
~/Portfolio/src/portfolio/dagster_enable_daemon.sh
~/Portfolio/src/portfolio/dagster_enable_server.sh
```

Data will now flow through as follows, with all steps managed by the orchestrator:
- Generated at "source" (data generator)
- Pushed to Postgres by source system team script (prod source replica)
- Acquired by Data Engineering owned CDC process, landing in ADLS2
- Propagated to silver and gold layers via DBT jobs executing in Snowflake